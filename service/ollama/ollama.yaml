---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-healthcheck
  namespace: ollama
data:
  # Proxy Config.yaml: https://docs.litellm.ai/docs/proxy/configs
  healthcheck.sh: |
    #/bin/bash
    set -eoux pipefail

    export OLLAMA_HOST=localhost:11434
    ollama ps | grep "$MODEL_NAME"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: ollama
spec:
  replicas: 1
  strategy:
    type: Recreate  # deploymentの更新時にPodを再起動する
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
        - name: ollama
          # DockerHub: https://hub.docker.com/r/ollama/ollama
          image: ollama/ollama:0.11.3
          ports:
            - containerPort: 11434
              protocol: TCP
          command:
            - /bin/bash
            - -c
            - |
              set -euo pipefail
              update-ca-certificates
              /bin/ollama serve
          env:
            - name: OLLAMA_FLASH_ATTENTION
              value: "1"
            - name: OLLAMA_KEEP_ALIVE
              value: "-1"
            # 同時リクエスト受付数
            # NOTE:
            #  同時リクエストには大量のKVキャッシュが必要になるため、以下のブログを参考にして試算すること
            #  https://developer.nvidia.com/ja-jp/blog/mastering-llm-techniques-inference-optimization/
            - name: OLLAMA_NUM_PARALLEL
              value: "1"
            - name: MODEL_NAME
              # https://ollama.com/library/deepseek-r1:14b-qwen-distill-q4_K_M
              value: "deepseek-r1:14b-qwen-distill-q4_K_M"  # 9GB
              # https://ollama.com/library/gpt-oss:120b
              # value: gpt-oss:120b  # 65GB
              # https://ollama.com/library/gpt-oss:20b
              # value: gpt-oss:20b  # 14GB
          volumeMounts:
            - name: ollama-healthcheck-volume
              mountPath: /healthcheck.sh
              subPath: healthcheck.sh
            - name: ollama-models  # ホストの /mnt/ollama-models ディレクトリをPodの /root/.ollama にマウント
              mountPath: /root/.ollama 
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - /healthcheck.sh
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            failureThreshold: 3
            timeoutSeconds: 3
          resources:
            requests:
              cpu: "1"
              memory: "4Gi"
              # 1台のGPUに載りきらない場合は複数のGPUに分散される: https://github.com/ollama/ollama/blob/main/docs/faq.md#how-does-ollama-load-models-on-multiple-gpus
              nvidia.com/gpu: "1"
            limits:
              cpu: "1"
              memory: "4Gi"
              nvidia.com/gpu: "1"
          lifecycle:
            postStart:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    export OLLAMA_HOST=localhost:11434
                    while ! /bin/ollama ps > /dev/null 2>&1; do
                      sleep 5
                    done
                    /bin/ollama pull $MODEL_NAME
                    /bin/ollama run $MODEL_NAME hello
      volumes:
        - name: ollama-healthcheck-volume
          configMap:
            name: ollama-healthcheck
        - name: ollama-models
          hostPath:  # ホストの /mnt/ollama-models ディレクトリをボリュームとして使用
            path: /mnt/ollama-models
            type: DirectoryOrCreate
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: eks.amazonaws.com/compute-type
                    operator: In
                    values:
                      - hybrid
---
apiVersion: v1
kind: Service
metadata:
  name: ollama-svc
  namespace: ollama
spec:
  type: NodePort
  selector:
    app: ollama
  ports:
    - port: 80
      protocol: TCP
      targetPort: 11434
      name: http
      nodePort: 30001
